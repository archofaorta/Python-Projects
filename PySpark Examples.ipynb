{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# $example on:spark_hive$\n",
    "from os.path import expanduser, join, abspath\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, asc\n",
    "from pyspark.sql import Row\n",
    "\n",
    "warehouse_location = abspath(\"C:/Users/sardara1/spark-warehouse\")\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 6)\n",
    "# spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "\n",
    "zipsDF = spark.read.json(\"zips.json\")\n",
    "zipsDF.filter(zipsDF['pop'] > 40000).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# $example on:spark_hive$\n",
    "from os.path import expanduser, join, abspath\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "warehouse_location = abspath(\"C:/Users/sardara1/spark-warehouse\")\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 6)\n",
    "# spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "\n",
    "zipsDF = spark.read.json(\"zips.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipsDF = spark.read.json(\"zips.json\").withColumnRenamed(\"_id\",\"zip\")\n",
    "zipsDF.filter(zipsDF['pop'] > 40000).show(10)\n",
    "zipsDF.createOrReplaceTempView(\"zips_table\")\n",
    "zipsDF.cache()\n",
    "resultDF = spark.sql(\"SELECT city, pop, state, zip from zips_table\")\n",
    "resultDF.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS zips_hive_table\")\n",
    "spark.table(\"zips_table\").write.saveAsTable(\"zips_hive_table\")\n",
    "resultsHiveDF = spark.sql(\"SELECT city, pop, state, zip FROM zips_hive_table WHERE pop > 40000\")\n",
    "resultsHiveDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsHiveDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(zip), SUM(pop), city FROM zips_hive_table WHERE state = 'CA' GROUP BY city ORDER BY SUM(pop) DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "lines = spark.sparkContext.textFile(\"file:///Users/sardara1/Downloads/New Learning/BigDataDatasets/ml-100k/u.data\")\n",
    "ratings = lines.map(lambda x: x.split()[2])\n",
    "result = ratings.countByValue()\n",
    "\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|movie_id\tuser_id\trating\ttime_stamp|\n",
      "+----------------------------------+\n",
      "|               196\t242\t3\t881250949|\n",
      "|               186\t302\t3\t891717742|\n",
      "|                22\t377\t1\t878887116|\n",
      "|                244\t51\t2\t880606923|\n",
      "|               166\t346\t1\t886397596|\n",
      "|               298\t474\t4\t884182806|\n",
      "|               115\t265\t2\t881171488|\n",
      "|               253\t465\t5\t891628467|\n",
      "|               305\t451\t3\t886324817|\n",
      "|                  6\t86\t3\t883603013|\n",
      "+----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'toDS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-44231088212c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1398\u001b[0m         \"\"\"\n\u001b[0;32m   1399\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1400\u001b[1;33m             raise AttributeError(\n\u001b[0m\u001b[0;32m   1401\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0;32m   1402\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'toDS'"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .option(\"delimiter\", \" \") \\\n",
    "    .load(\"file:///Users/sardara1/Downloads/New Learning/BigDataDatasets/ml-100k/u.data\")\n",
    "\n",
    "lines = df.rdd\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-30-51ff8be65680>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-30-51ff8be65680>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    ratings = lines.map(lambda x: Row(ratings = int(x:x[0]split()[2])))\u001b[0m\n\u001b[1;37m                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ratings = lines.map(lambda x: Row(ratings = int(x.split()[2])))\n",
    "\n",
    "#crdd = categories.rdd.map(lambda line: line.value.split(',')[1])\n",
    "\n",
    "result = ratings.countByValue()\n",
    "\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\sardara1\\\\Documents\\\\My Received Files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
